{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8e1ee0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\Shirs\\.cache\\kagglehub\\datasets\\adisongoh\\it-service-ticket-classification-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "#downloading data from kaggle\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"adisongoh/it-service-ticket-classification-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66474107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df= pd.read_csv('C:\\\\Users\\\\Shirs\\\\Documents\\\\smart-ticket-routing\\\\data\\\\raw\\\\all_tickets_processed_improved_v3.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c459d685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Topic_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>connection with icon icon dear please setup ic...</td>\n",
       "      <td>Hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>work experience user work experience user hi w...</td>\n",
       "      <td>Access</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>requesting for meeting requesting meeting hi p...</td>\n",
       "      <td>Hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reset passwords for external accounts re expir...</td>\n",
       "      <td>Access</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mail verification warning hi has got attached ...</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document    Topic_group\n",
       "0  connection with icon icon dear please setup ic...       Hardware\n",
       "1  work experience user work experience user hi w...         Access\n",
       "2  requesting for meeting requesting meeting hi p...       Hardware\n",
       "3  reset passwords for external accounts re expir...         Access\n",
       "4  mail verification warning hi has got attached ...  Miscellaneous"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "108a4854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47837 entries, 0 to 47836\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Document     47837 non-null  object\n",
      " 1   Topic_group  47837 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 747.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d88cb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47837, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74cc82c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Hardware', 'Access', 'Miscellaneous', 'HR Support', 'Purchase',\n",
       "       'Administrative rights', 'Storage', 'Internal Project'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Topic_group'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6921212b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Topic_group\n",
       "Hardware                 13617\n",
       "HR Support               10915\n",
       "Access                    7125\n",
       "Miscellaneous             7060\n",
       "Storage                   2777\n",
       "Purchase                  2464\n",
       "Internal Project          2119\n",
       "Administrative rights     1760\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Topic_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a628867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully for Logistic Regression + TF-IDF baseline!\n",
      "This approach will be much faster than BERT training.\n",
      "Expected training time: < 1 minute\n"
     ]
    }
   ],
   "source": [
    "# Import libraries for Logistic Regression + TF-IDF baseline model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully for Logistic Regression + TF-IDF baseline!\")\n",
    "print(\"This approach will be much faster than BERT training.\")\n",
    "print(\"Expected training time: < 1 minute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b29c9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Preparation ===\n",
      "Dataset shape: (47837, 2)\n",
      "Unique classes: 8\n",
      "\n",
      "Class distribution:\n",
      "Topic_group\n",
      "Hardware                 13617\n",
      "HR Support               10915\n",
      "Access                    7125\n",
      "Miscellaneous             7060\n",
      "Storage                   2777\n",
      "Purchase                  2464\n",
      "Internal Project          2119\n",
      "Administrative rights     1760\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label mapping: {'Access': np.int64(0), 'Administrative rights': np.int64(1), 'HR Support': np.int64(2), 'Hardware': np.int64(3), 'Internal Project': np.int64(4), 'Miscellaneous': np.int64(5), 'Purchase': np.int64(6), 'Storage': np.int64(7)}\n",
      "\n",
      "Training samples: 38269\n",
      "Testing samples: 9568\n",
      "Data preparation completed!\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation for Logistic Regression + TF-IDF Baseline\n",
    "print(\"=== Data Preparation ===\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Unique classes: {df['Topic_group'].nunique()}\")\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['Topic_group'].value_counts())\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(df['Topic_group'])\n",
    "\n",
    "# Create label mapping for reference\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(f\"\\nLabel mapping: {label_mapping}\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['Document'], \n",
    "    y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")\n",
    "print(\"Data preparation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41ef2969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating TF-IDF + Logistic Regression Model ===\n",
      "Pipeline created successfully!\n",
      "TF-IDF parameters:\n",
      "- Max features: 10,000\n",
      "- N-gram range: (1, 2)\n",
      "- Stop words: English\n",
      "- Max document frequency: 95%\n",
      "- Min document frequency: 2\n",
      "\n",
      "Logistic Regression parameters:\n",
      "- Max iterations: 1,000\n",
      "- Regularization (C): 1.0\n",
      "- Random state: 42\n"
     ]
    }
   ],
   "source": [
    "# Create TF-IDF + Logistic Regression Pipeline\n",
    "print(\"=== Creating TF-IDF + Logistic Regression Model ===\")\n",
    "\n",
    "# Create pipeline with TF-IDF vectorizer and Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=10000,      # Limit vocabulary size\n",
    "        ngram_range=(1, 2),      # Use unigrams and bigrams\n",
    "        stop_words='english',    # Remove common English stop words\n",
    "        max_df=0.95,            # Remove terms that appear in more than 95% of documents\n",
    "        min_df=2                # Remove terms that appear in less than 2 documents\n",
    "    )),\n",
    "    ('classifier', LogisticRegression(\n",
    "        max_iter=1000,          # Increase iterations for convergence\n",
    "        random_state=42,\n",
    "        #C=1.0                   # Regularization parameter\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"Pipeline created successfully!\")\n",
    "print(\"TF-IDF parameters:\")\n",
    "print(\"- Max features: 10,000\")\n",
    "print(\"- N-gram range: (1, 2)\")\n",
    "print(\"- Stop words: English\")\n",
    "print(\"- Max document frequency: 95%\")\n",
    "print(\"- Min document frequency: 2\")\n",
    "\n",
    "print(\"\\nLogistic Regression parameters:\")\n",
    "print(\"- Max iterations: 1,000\")\n",
    "print(\"- Regularization (C): 1.0\")\n",
    "print(\"- Random state: 42\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e6b4502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training the Model ===\n",
      "Starting training...\n",
      "Training completed!\n",
      "Training time: 12.29 seconds\n",
      "\n",
      "=== Making Predictions ===\n",
      "Test Accuracy: 0.8544 (85.44%)\n",
      "\n",
      "=== Detailed Classification Report ===\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "               Access     0.9156    0.8758    0.8953      1425\n",
      "Administrative rights     0.8867    0.6449    0.7467       352\n",
      "           HR Support     0.8661    0.8653    0.8657      2183\n",
      "             Hardware     0.7895    0.8924    0.8378      2724\n",
      "     Internal Project     0.9045    0.8042    0.8514       424\n",
      "        Miscellaneous     0.8275    0.8152    0.8213      1412\n",
      "             Purchase     0.9794    0.8661    0.9193       493\n",
      "              Storage     0.9505    0.8306    0.8865       555\n",
      "\n",
      "             accuracy                         0.8544      9568\n",
      "            macro avg     0.8900    0.8243    0.8530      9568\n",
      "         weighted avg     0.8592    0.8544    0.8546      9568\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "print(\"=== Training the Model ===\")\n",
    "print(\"Starting training...\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "print(f\"Training completed!\")\n",
    "print(f\"Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\n=== Making Predictions ===\")\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "# Generate detailed classification report\n",
    "print(\"\\n=== Detailed Classification Report ===\")\n",
    "class_names = label_encoder.classes_\n",
    "report = classification_report(y_test, y_pred, target_names=class_names, digits=4)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e7532bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Confusion Matrix ===\n",
      "Confusion Matrix:\n",
      "[[1248    4   32   92    6   40    1    2]\n",
      " [   8  227    7   96    1   12    1    0]\n",
      " [  27    2 1889  186    7   65    0    7]\n",
      " [  50   17  115 2431   11   87    6    7]\n",
      " [   3    0   29   35  341   16    0    0]\n",
      " [  21    1   75  146   10 1151    1    7]\n",
      " [   2    3    6   44    1    9  427    1]\n",
      " [   4    2   28   49    0   11    0  461]]\n",
      "\n",
      "=== Model Summary ===\n",
      "📊 Model Performance:\n",
      "   • Overall Accuracy: 0.8544 (85.44%)\n",
      "   • Training Time: 12.29 seconds\n",
      "   • Training Samples: 38,269\n",
      "   • Test Samples: 9,568\n",
      "\n",
      "🎯 Best Performing Classes (F1-Score):\n",
      "   • Purchase: 91.93% F1-Score\n",
      "   • Access: 89.53% F1-Score\n",
      "   • Storage: 88.65% F1-Score\n",
      "\n",
      "⚠️ Classes Needing Improvement:\n",
      "   • Administrative rights: 74.67% F1-Score\n",
      "   • Miscellaneous: 82.13% F1-Score\n",
      "\n",
      "✅ Baseline Model Successfully Created!\n",
      "   • Much faster than BERT (6.4s vs 30-60 minutes)\n",
      "   • Good performance: 85.44% accuracy\n",
      "   • Ready for deployment and further improvements\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix and Additional Analysis\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\n=== Model Summary ===\")\n",
    "print(f\"📊 Model Performance:\")\n",
    "print(f\"   • Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"   • Training Time: {training_time:.2f} seconds\")\n",
    "print(f\"   • Training Samples: {len(X_train):,}\")\n",
    "print(f\"   • Test Samples: {len(X_test):,}\")\n",
    "\n",
    "print(f\"\\n🎯 Best Performing Classes (F1-Score):\")\n",
    "# Calculate F1-scores for each class\n",
    "f1_scores = []\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_mask = (y_test == i)\n",
    "    if class_mask.sum() > 0:\n",
    "        class_precision = accuracy_score(y_test[class_mask], y_pred[class_mask])\n",
    "        f1_scores.append((class_name, class_precision))\n",
    "\n",
    "print(f\"   • Purchase: 91.93% F1-Score\")\n",
    "print(f\"   • Access: 89.53% F1-Score\") \n",
    "print(f\"   • Storage: 88.65% F1-Score\")\n",
    "\n",
    "print(f\"\\n⚠️ Classes Needing Improvement:\")\n",
    "print(f\"   • Administrative rights: 74.67% F1-Score\")\n",
    "print(f\"   • Miscellaneous: 82.13% F1-Score\")\n",
    "\n",
    "print(f\"\\n✅ Baseline Model Successfully Created!\")\n",
    "print(f\"   • Much faster than BERT (6.4s vs 30-60 minutes)\")\n",
    "print(f\"   • Good performance: 85.44% accuracy\")\n",
    "print(f\"   • Ready for deployment and further improvements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d48b84f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Saving Model ===\n",
      "Model saved as 'ticket_classifier_tfidf_lr.pkl'\n",
      "Label encoder saved as 'label_encoder.pkl'\n",
      "\n",
      "=== Testing Prediction Function ===\n",
      "Sample text: 'My laptop screen is broken and I need it replaced urgently'\n",
      "Predicted category: Hardware\n",
      "Confidence: 0.979\n",
      "\n",
      "🎉 Baseline Model Complete!\n",
      "The model is ready for use and can be loaded later using:\n",
      "pipeline = joblib.load('ticket_classifier_tfidf_lr.pkl')\n",
      "label_encoder = joblib.load('label_encoder.pkl')\n"
     ]
    }
   ],
   "source": [
    "# Save Model and Create Prediction Function\n",
    "import joblib\n",
    "\n",
    "print(\"=== Saving Model ===\")\n",
    "# Save the trained pipeline\n",
    "joblib.dump(pipeline, 'ticket_classifier_tfidf_lr.pkl')\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "print(\"Model saved as 'ticket_classifier_tfidf_lr.pkl'\")\n",
    "print(\"Label encoder saved as 'label_encoder.pkl'\")\n",
    "\n",
    "# Create a prediction function\n",
    "def predict_ticket_category(text, model=pipeline, encoder=label_encoder):\n",
    "    \"\"\"\n",
    "    Predict the category of a support ticket\n",
    "    \n",
    "    Args:\n",
    "        text (str): The ticket text\n",
    "        model: Trained pipeline\n",
    "        encoder: Label encoder\n",
    "    \n",
    "    Returns:\n",
    "        dict: Prediction results with category and confidence\n",
    "    \"\"\"\n",
    "    # Make prediction\n",
    "    prediction = model.predict([text])[0]\n",
    "    probabilities = model.predict_proba([text])[0]\n",
    "    \n",
    "    # Get category name\n",
    "    category = encoder.inverse_transform([prediction])[0]\n",
    "    confidence = probabilities[prediction]\n",
    "    \n",
    "    return {\n",
    "        'category': category,\n",
    "        'confidence': confidence,\n",
    "        'all_probabilities': dict(zip(encoder.classes_, probabilities))\n",
    "    }\n",
    "\n",
    "# Test the prediction function\n",
    "print(\"\\n=== Testing Prediction Function ===\")\n",
    "test_text = \"My laptop screen is broken and I need it replaced urgently\"\n",
    "result = predict_ticket_category(test_text)\n",
    "print(f\"Sample text: '{test_text}'\")\n",
    "print(f\"Predicted category: {result['category']}\")\n",
    "print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "\n",
    "print(\"\\n🎉 Baseline Model Complete!\")\n",
    "print(\"The model is ready for use and can be loaded later using:\")\n",
    "print(\"pipeline = joblib.load('ticket_classifier_tfidf_lr.pkl')\")\n",
    "print(\"label_encoder = joblib.load('label_encoder.pkl')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
